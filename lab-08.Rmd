---
title: "Lab 08 - Predicting rain"
author: "Muhammad Sami Munawar"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
```

```{r}
weather <- read.csv("data/weatherAUS.csv", header = TRUE)
```

# Exercise 1: Exploratory Data Analysis

We will start by transform any character variables that need to be transformed into categorical. Use the following code to identify them and trasform them into factors.

```{r mutate-char}
variables_to_transform = weather %>% 
  select(where(is.character),-Date) %>% names()
weather <- weather %>% 
  mutate_at(vars(all_of(variables_to_transform)),factor)

```

To simplify things, today we will not be using some categorical explanatory variables, because they have a very large number of categories and might make our model interpretation more complex. Specifically we will exclude `WindGustDir`, `WindDir9am` and `WindDir3pm`. 

```{r remove-wind}
weather <- weather %>%
  select(-WindGustDir,-WindDir9am,-WindDir3pm)
```

Note that some of these variables have a large number of missing values:

```{r find-prop-na}
weather %>% 
  select(where(~ any(is.na(.)))) %>% 
  summarise(across(everything(), ~mean(is.na(.)))) %>%
  pivot_longer(col = everything(), names_to = "Variable", values_to = "Prop_NA") %>%
  arrange(desc(Prop_NA))
```

1. Are there any missing values in our variable of interest `RainTomorrow`? If so, we filter them out and save the new dataset as `weather_noNA`. 

```{r}
weather_noNA <- weather %>% filter(!is.na(RainTomorrow))

```

2. Which cities are the ones with more rain days? To do this, let's analyze the `RainToday` variable. 

```{r}
rain_days_by_city <- weather_noNA %>%
                     group_by(Location) %>%
                     summarise(RainDays = sum(RainToday == "Yes", na.rm = TRUE)) %>%
                     arrange(desc(RainDays))

```

# Exercise 2: Logistic regression

We will focus our analysis on the city of `Portland`.

```{r}
weather_Portland <- weather %>%
  filter(Location == "Portland")
```

1. Try to predict `RainTomorrow` by fitting a linear regression using the variable `RainToday` and print the output using `tidy()`.

```{r}
model <- lm(RainTomorrow ~ RainToday, data = weather_Portland)
tidy_output <- broom::tidy(model)
print(tidy_output)


```

2. For each point in our dataset, what are the fitted probabilities that tomorrow it's going to rain? 

- Plot them using an appropriate visualization. What is peculiar about them?

```{r}

fitted_probs <- predict(model, weather_Portland, type = "response")

fitted_probs_constrait <- pmin(pmax(fitted_probs, 0), 1)  # Constrain values to [0, 1]

prob_data <- data.frame(PredictedProbability = fitted_probs_constrait)

ggplot(prob_data, aes(x = PredictedProbability, y = 1)) +
  geom_jitter(width = 0.1, height = 0, color = "blue") +
  labs(x = "Predicted Probability", y = "Observations", title = "Fitted Probabilities for Rain Tomorrow") 

```

> Hint: how many unique values do the predicted probabilities take? What do these value correspond to?


- Are there any missing values? Why?

```{r}

```

If there are any missing values in "RainToday" or "RainTomorrow", they would have been excluded from the model fitting.

# Exercise 3: Split the data and build workflows

Let us set a seed and perform a split of our data.

```{r seed}
set.seed(111723)
```

1. Split the data into a training set (80% of your Portland data) and a testing set.

```{r}
data_split <- initial_split(weather_Portland, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

```

2. Refit the simple logistic regression using `RainToday` as predictor on this training data, using `tidymodels` recipes and workflows.

- Start by the recipe. First initialize the recipe, then remove observations with missing values using `step_naomit()` and finally use `step_dummy` to convert categorical to dummy variables.

```{r recipe1, eval=FALSE}
# remove eval=FALSE
weather_rec1 <- recipe(
  RainTomorrow ~ RainToday, 
  data = train_data
) %>%
  step_naomit(all_predictors()) %>%
  step_dummy(all_nominal(), -all_outcomes())

logit_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Create the workflow
logit_workflow <- workflow() %>%
  add_recipe(weather_rec1) %>%
  add_model(logit_spec)

# Fit the model to the training data
logit_fit <- logit_workflow %>%
  fit(data = train_data)


```

- Build your workflow combining model and recipe

```{r workflow1, eval = FALSE}
# remove eval=FALSE
weather_mod1 <- logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")
weather_wflow1 <- workflow() %>%  # initiate workflow
  add_model(weather_mod1) %>%     # add model
  add_recipe(weather_rec1)        # add recipe
```

- Now fit your model on the training data

```{r fit1, eval = FALSE}
# remove eval=FALSE
weather_fit1 <- weather_wflow1 %>% 
  fit(data = train_data)
tidy(weather_fit1)

```

3. Fit now a multiple logistic regression, i.e. using multiple explanatory variables, to predict 
`RainTomorrow`. We will use as predictors the variables `MinTemp`, `MaxTemp`, `RainToday` and `Rainfall`. Similarly to question 2, use workflows to fit this model on our training data. 

- Start by the recipe. This will be a simple recipe, because we have not done many pre-processing steps, but do remove missing values and transform categorical variables into dummy variables:

```{r recipe2, eval=FALSE}
# remove eval=FALSE
weather_rec2 <- recipe(
  RainTomorrow ~ MinTemp + MaxTemp + RainToday + Rainfall, 
  data = train_data  # Make sure this is your training data
) %>%
  step_naomit(all_predictors(), all_outcomes()) %>%  # Notice the pipe here
  step_dummy(all_nominal(), -all_outcomes())  # Transform categorical variables, excluding the outcome


```

- Save the model, workflow and finally, let's fit to the training data.

```{r}


logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")


weather_recipe <- recipe(
  RainTomorrow ~ MinTemp + MaxTemp + RainToday + Rainfall, 
  data = train_data
) %>%
  step_naomit(all_predictors(), all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())


weather_workflow <- workflow() %>%
  add_model(logistic_spec) %>%
  add_recipe(weather_recipe)


weather_fit <- weather_workflow %>%
  fit(data = train_data)


saveRDS(weather_fit, file = "weather_fit.rds")

```

3. Now let's evaluate the predictive performance of these two models on our test set.

- Create the ROC curve and get the AUC (area under the curve) value for your first simple logistic regression model.

```{r eval=FALSE}

weather_pred1 <- predict(weather_fit1, test_data, type = "prob") %>%
  bind_cols(test_data)


roc_curve1 <- roc_curve(
  data = weather_pred1,
  truth = RainTomorrow,
  .pred_Yes,
  event_level = "second"
)


autoplot(roc_curve1)


auc1 <- roc_auc(
  data = weather_pred1,
  truth = RainTomorrow,
  .pred_Yes,
  event_level = "second"
)


(auc1)

```

- Create now the ROC curve and get the AUC (area under the curve) value for your second model.

```{r}

# Make predictions on the test set
weather_pred2 <- predict(weather_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

# Calculate the ROC curve for the second model
roc_curve2 <- roc_curve(
  data = weather_pred2,
  truth = RainTomorrow,
  .pred_Yes,
  event_level = "second"
)

# Plot ROC curve
autoplot(roc_curve2)

# Calculate AUC for the second model
auc2 <- roc_auc(
  data = weather_pred2,
  truth = RainTomorrow,
  .pred_Yes,
  event_level = "second"
)

# Print AUC value
print(auc2)


```

- Which model seems to have a better performance?

the second model, (model 2)


4. Now focus on the second model. Consider several thresholds for predicting `RainTomorrow` and look at the number of false positives and false negatives. For example:

```{r}
cutoff_prob <- 0.5
weather_pred2 %>%
  mutate(
    RainTomorrow      = if_else(RainTomorrow == "Yes", "It rains", "It does not rain"),
    RainTomorrow_pred = if_else(.pred_Yes > cutoff_prob, "Predicted rain", "Predicted no rain")
    ) %>%
  na.omit() %>%
  count(RainTomorrow_pred, RainTomorrow)


```

- What is the the false positive rate with `cutoff_prob = 0.3`? 

FPR= (False Positives+True Negatives) / False Positives

The false positive rate (FPR) with a cutoff probability of 0.3 is approximately 0.220 or 22.0%


# Exercise 4: Extend our model [OPTIONAL]

We will now try to improve our fit by building a model using additional explanatory variables.

1. Let us analyze the various variables in our dataset.

- Is there any categorical variable which is very unbalanced? If so, remove it.

- Is there any numerical variable that has a very small standard deviation for one of the two categories of `RainTomorrow`? If so, remove it.

2. Let's do some feature engineering: let us transform the variable `Date`. We will use `Ludbridate` again: extract the month and year.

3. Let's now combine everything into recipes and workflows. Then fit the model on the training data and use the test data for calculating the AUC and plotting the ROC curve.

4. Is this model better than the one we fitted in Exercise 3?
